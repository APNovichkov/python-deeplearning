{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Imports\n",
    "import matplotlib.pyplot as plt #This package is for plotting\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Keras Imports\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import load_model\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Andrey Novichkov\n",
    "## Using Keras for linear/logistic regression on 2 datasets\n",
    "\n",
    "First, we want to build the simplest Keras NN and compare it to sklearn's output.  \n",
    "Then, we want to improve and optimize our Keras and Sklearn implementations to see how good we can make the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression w/ simple Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input data\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 1ms/step\n",
      "mse: 39.55\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "# input layer w/ 1 Neuron\n",
    "model.add(Dense(1, input_shape=(X_train.shape[1],)))\n",
    "# output layer with linear activation\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# Compile model with MSE loss and adam optimizer\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error'])\n",
    "# Fit the model with 100 epochs and batch size of 1, because dataset very small\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "loss, mse = model.evaluate(X_test, y_test)\n",
    "print(f'mse: {round(mse, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression w/ Sklearn\n",
    "We are going to use the same X_train, X_test and y_train and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 29.78\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr = lg.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'mse: {round(mse, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, sklearn's linear regression model performed better than the keras one with the parameters that I used for building the Keras NN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression w/ simple Keras\n",
    "Going to use diabetes.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "y = df['Outcome'].to_numpy()\n",
    "X = df.drop('Outcome', axis=1).to_numpy()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136379e90>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1, input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 92us/step\n",
      "accuracy: 75.52%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'accuracy: {round(accuracy*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ Sklearn\n",
    "Going to use same training data as with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 79.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression()\n",
    "lg = lg.fit(X_train, y_train)\n",
    "y_pred = lg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'accuracy: {round(accuracy*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, logistic regression was also better with sklearn without any optimizations done on either the simple Keras NN and sklearn models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's optimize both linear and logistic regression for Keras and Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Linear Regression for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input data\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "379/379 [==============================] - 1s 3ms/step - loss: 1537.0363 - mean_squared_error: 1537.0363\n",
      "Epoch 2/20\n",
      "379/379 [==============================] - 1s 1ms/step - loss: 99.3677 - mean_squared_error: 99.3677\n",
      "Epoch 3/20\n",
      "379/379 [==============================] - 0s 948us/step - loss: 74.9715 - mean_squared_error: 74.9715\n",
      "Epoch 4/20\n",
      "379/379 [==============================] - 0s 1ms/step - loss: 82.2205 - mean_squared_error: 82.2205\n",
      "Epoch 5/20\n",
      "379/379 [==============================] - 0s 1ms/step - loss: 79.1612 - mean_squared_error: 79.1612\n",
      "Epoch 6/20\n",
      "379/379 [==============================] - 0s 1ms/step - loss: 79.3991 - mean_squared_error: 79.3991\n",
      "Epoch 7/20\n",
      "379/379 [==============================] - 1s 1ms/step - loss: 63.8141 - mean_squared_error: 63.8141\n",
      "Epoch 8/20\n",
      "379/379 [==============================] - 0s 959us/step - loss: 73.7362 - mean_squared_error: 73.7362\n",
      "Epoch 9/20\n",
      "379/379 [==============================] - 0s 896us/step - loss: 77.8874 - mean_squared_error: 77.8874\n",
      "Epoch 10/20\n",
      "379/379 [==============================] - 0s 887us/step - loss: 70.8530 - mean_squared_error: 70.8530\n",
      "Epoch 11/20\n",
      "379/379 [==============================] - 0s 869us/step - loss: 62.7077 - mean_squared_error: 62.7077\n",
      "Epoch 12/20\n",
      "379/379 [==============================] - 0s 901us/step - loss: 51.7565 - mean_squared_error: 51.7565\n",
      "Epoch 13/20\n",
      "379/379 [==============================] - 0s 889us/step - loss: 57.9785 - mean_squared_error: 57.9785\n",
      "Epoch 14/20\n",
      "379/379 [==============================] - 0s 867us/step - loss: 72.3305 - mean_squared_error: 72.3305\n",
      "Epoch 15/20\n",
      "379/379 [==============================] - 0s 889us/step - loss: 66.1702 - mean_squared_error: 66.1702\n",
      "Epoch 16/20\n",
      "379/379 [==============================] - 0s 862us/step - loss: 73.0459 - mean_squared_error: 73.0459\n",
      "Epoch 17/20\n",
      "379/379 [==============================] - 0s 877us/step - loss: 59.8577 - mean_squared_error: 59.8577\n",
      "Epoch 18/20\n",
      "379/379 [==============================] - 0s 933us/step - loss: 56.7450 - mean_squared_error: 56.7450\n",
      "Epoch 19/20\n",
      "379/379 [==============================] - 0s 874us/step - loss: 58.4767 - mean_squared_error: 58.4767\n",
      "Epoch 20/20\n",
      "379/379 [==============================] - 0s 858us/step - loss: 62.1388 - mean_squared_error: 62.1388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1365c0f90>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
