{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Review - 06/07/20\n",
    "### Andrey Novichkov\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "- This is a computational system inspired by structure, processing method and learning ability similar to our human brain  \n",
    "\n",
    "A Neural Network is made up of 3 unique types of structures: **Inputs**, **Hidden Layers** and **Outputs**.  \n",
    "\n",
    "**Inputs**: Are what is given to the network  \n",
    "**Outputs**: What comes out of the neural network  \n",
    "**Hidden Layers**: These are layers of sometimes multiple nodes, where each node is a **perceptron**.  \n",
    "**Perceptron**: This is a node that mimics a neuron in the human body.   \n",
    "\n",
    "### What is a Perceptron:\n",
    "It is made out of several parts: **Inputs** + **Weights** -> **Summation and Bias** -> **Activation f(x)** -> **1 output**  \n",
    "  \n",
    "**Summation and Bias**: This is the function that combines inputs and weights  \n",
    "**Activation f(x)**: A function that determines the output, could be a threshold function, but could be something else\n",
    "\n",
    "\n",
    "### Forward and Backward Propagation\n",
    "When we want the neural net to make a prediction, we give it some input, which runs through the input layer, the hidden layer(s) and is presented as output via the output layer, this process is called **forward propagation**  \n",
    "  \n",
    "When we get the output layer, we compare that result to the actual answer. This step is usually defined by a **cost function**, and we want our neural net to have as small of an error as possible  \n",
    "  \n",
    "We then run back through the neural net and update our weights(gradient descent) in order to minimize that cost function. This process is called **back propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement a very simple Neural Net ourselves, and see the steps in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "# Output array\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "# Sigmoid Function\n",
    "# This is to introduce non-linearity to the activation function (output of NN)\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's initialize the variables we need to build our neural network**  \n",
    "We are only going to have 1 hidden layer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training iterations\n",
    "epoch=5000 \n",
    "# Setting learning rate\n",
    "lr=0.1 \n",
    "# Number of features = number of inputs into NN = # columns in dataset\n",
    "inputlayer_neurons = X.shape[1] \n",
    "# How many hidden layer neurons we have. This is defined by us\n",
    "hiddenlayer_neurons = 3\n",
    "# Number of outputs. This is also defined by us and the dataset we have\n",
    "output_neurons = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to initialize weights and bias' for the NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wh: [[0.66571579 0.89040976 0.48733742]\n",
      " [0.96375525 0.10898274 0.30842886]\n",
      " [0.00141498 0.07281419 0.55748999]\n",
      " [0.61820128 0.59712594 0.61647234]]\n",
      "bh: [[0.41379415 0.79752472 0.5068145 ]]\n"
     ]
    }
   ],
   "source": [
    "# For input to HIDDEN layer\n",
    "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))\n",
    "bh = np.random.uniform(size=(1, hiddenlayer_neurons))\n",
    "\n",
    "print(f'wh: {wh}')\n",
    "print(f'bh: {bh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wout: [[0.6907534 ]\n",
      " [0.98427091]\n",
      " [0.54267795]]\n",
      "bout: [[0.64192893]]\n"
     ]
    }
   ],
   "source": [
    "# For input to OUTPUT layer\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "print(f'wout: {wout}')\n",
    "print(f'bout: {bout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are ready for the training part**  \n",
    "Again, this consists of forward and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    # Forward Propagation\n",
    "    # Input Layer\n",
    "    hidden_layer_input = np.dot(X, wh)\n",
    "    hidden_layer_input = hidden_layer_input + bh\n",
    "    hidden_layer_activations = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Hidden Layer\n",
    "    output_layer_input = np.dot(hidden_layer_activations, wout)\n",
    "    output_layer_input = output_layer_input + bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backward Propagation - Optimizing cost function -> Read more about this\n",
    "    error = y - output\n",
    "\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n",
    "\n",
    "    d_output = error * slope_output_layer\n",
    "    d_hidden_layer = d_output.dot(wout.T) * slope_hidden_layer\n",
    "\n",
    "    wout += hidden_layer_activations.T.dot(d_output) * lr\n",
    "    bout += np.sum(d_output, axis=0, keepdims=True) * lr\n",
    "\n",
    "    wh += X.T.dot(d_hidden_layer) * lr\n",
    "    bh += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct output: [[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Correct output: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Outp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
